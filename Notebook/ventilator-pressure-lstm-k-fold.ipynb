{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ff88c3",
   "metadata": {
    "papermill": {
     "duration": 0.010786,
     "end_time": "2021-10-30T13:56:07.413666",
     "exception": false,
     "start_time": "2021-10-30T13:56:07.402880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b20152e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-30T13:56:07.524940Z",
     "iopub.status.busy": "2021-10-30T13:56:07.524036Z",
     "iopub.status.idle": "2021-10-30T13:56:24.561401Z",
     "shell.execute_reply": "2021-10-30T13:56:24.560444Z",
     "shell.execute_reply.started": "2021-10-30T08:40:49.800348Z"
    },
    "papermill": {
     "duration": 17.139166,
     "end_time": "2021-10-30T13:56:24.561562",
     "exception": false,
     "start_time": "2021-10-30T13:56:07.422396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected = True)\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "import optuna\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#########################################################\n",
    "train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\n",
    "test = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\n",
    "ss = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52af47bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-30T13:56:24.583778Z",
     "iopub.status.busy": "2021-10-30T13:56:24.583102Z",
     "iopub.status.idle": "2021-10-30T13:56:28.389548Z",
     "shell.execute_reply": "2021-10-30T13:56:28.389054Z",
     "shell.execute_reply.started": "2021-10-30T08:41:06.754143Z"
    },
    "papermill": {
     "duration": 3.819365,
     "end_time": "2021-10-30T13:56:28.389678",
     "exception": false,
     "start_time": "2021-10-30T13:56:24.570313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-30 13:56:25.068593: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler, normalize\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57532395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-30T13:56:28.432805Z",
     "iopub.status.busy": "2021-10-30T13:56:28.431643Z",
     "iopub.status.idle": "2021-10-30T13:57:16.173336Z",
     "shell.execute_reply": "2021-10-30T13:57:16.172421Z",
     "shell.execute_reply.started": "2021-10-30T08:41:10.780488Z"
    },
    "papermill": {
     "duration": 47.775002,
     "end_time": "2021-10-30T13:57:16.173480",
     "exception": false,
     "start_time": "2021-10-30T13:56:28.398478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df['area'] = df['time_step'] * df['u_in']\n",
    "    df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
    "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
    "    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n",
    "    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n",
    "    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n",
    "    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n",
    "    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n",
    "    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n",
    "    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n",
    "    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n",
    "    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n",
    "    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n",
    "    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n",
    "    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n",
    "    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n",
    "    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n",
    "    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n",
    "    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n",
    "    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n",
    "    \n",
    "    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n",
    "    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n",
    "    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n",
    "    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n",
    "    \n",
    "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
    "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
    "    \n",
    "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
    "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
    "    \n",
    "    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n",
    "    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n",
    "    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n",
    "    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n",
    "    df['cross']= df['u_in']*df['u_out']\n",
    "    df['cross2']= df['time_step']*df['u_out']\n",
    "    \n",
    "    df['R'] = df['R'].astype(str)\n",
    "    df['C'] = df['C'].astype(str)\n",
    "    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n",
    "    df = pd.get_dummies(df)\n",
    "    return df\n",
    "train = add_features(train)\n",
    "test = add_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34d23c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-30T13:57:16.196675Z",
     "iopub.status.busy": "2021-10-30T13:57:16.195544Z",
     "iopub.status.idle": "2021-10-30T13:57:20.325856Z",
     "shell.execute_reply": "2021-10-30T13:57:20.324939Z",
     "shell.execute_reply.started": "2021-10-30T08:41:58.152185Z"
    },
    "papermill": {
     "duration": 4.143241,
     "end_time": "2021-10-30T13:57:20.325994",
     "exception": false,
     "start_time": "2021-10-30T13:57:16.182753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2acd9718",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-30T13:57:20.964594Z",
     "iopub.status.busy": "2021-10-30T13:57:20.933055Z",
     "iopub.status.idle": "2021-10-30T13:57:21.618586Z",
     "shell.execute_reply": "2021-10-30T13:57:21.617668Z",
     "shell.execute_reply.started": "2021-10-30T08:42:02.258526Z"
    },
    "papermill": {
     "duration": 1.283766,
     "end_time": "2021-10-30T13:57:21.618735",
     "exception": false,
     "start_time": "2021-10-30T13:57:20.334969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets = train[['pressure']].to_numpy().reshape(-1, 80)\n",
    "train.drop(['pressure', 'id', 'breath_id'], axis = 1, inplace = True)\n",
    "test = test.drop(['id', 'breath_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4999c749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-30T13:57:21.644464Z",
     "iopub.status.busy": "2021-10-30T13:57:21.643361Z",
     "iopub.status.idle": "2021-10-30T13:57:31.822371Z",
     "shell.execute_reply": "2021-10-30T13:57:31.821789Z",
     "shell.execute_reply.started": "2021-10-30T08:42:03.523614Z"
    },
    "papermill": {
     "duration": 10.194981,
     "end_time": "2021-10-30T13:57:31.822514",
     "exception": false,
     "start_time": "2021-10-30T13:57:21.627533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RS = RobustScaler()\n",
    "train = RS.fit_transform(train)\n",
    "test = RS.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d764f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-30T13:57:31.844856Z",
     "iopub.status.busy": "2021-10-30T13:57:31.844117Z",
     "iopub.status.idle": "2021-10-30T13:57:31.846328Z",
     "shell.execute_reply": "2021-10-30T13:57:31.846696Z",
     "shell.execute_reply.started": "2021-10-30T08:42:13.635968Z"
    },
    "papermill": {
     "duration": 0.015361,
     "end_time": "2021-10-30T13:57:31.846849",
     "exception": false,
     "start_time": "2021-10-30T13:57:31.831488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.reshape(-1, 80, train.shape[-1])\n",
    "test = test.reshape(-1, 80, train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70eeb6eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-30T13:57:31.876622Z",
     "iopub.status.busy": "2021-10-30T13:57:31.876066Z",
     "iopub.status.idle": "2021-10-30T15:08:12.197117Z",
     "shell.execute_reply": "2021-10-30T15:08:12.198224Z",
     "shell.execute_reply.started": "2021-10-30T08:42:13.641457Z"
    },
    "papermill": {
     "duration": 4240.343216,
     "end_time": "2021-10-30T15:08:12.198510",
     "exception": false,
     "start_time": "2021-10-30T13:57:31.855294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 1 < ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-30 13:57:32.745104: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-30 13:57:32.747854: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-10-30 13:57:32.790954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-30 13:57:32.791689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-10-30 13:57:32.791764: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-30 13:57:32.791843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-30 13:57:32.791879: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-30 13:57:32.791953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-30 13:57:32.791998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-30 13:57:32.792031: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-30 13:57:32.792061: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-10-30 13:57:32.792098: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-10-30 13:57:32.792205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-30 13:57:32.792866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-30 13:57:32.794371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-30 13:57:32.794921: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-30 13:57:32.795173: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-30 13:57:32.795343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-30 13:57:32.795947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-10-30 13:57:32.796020: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-30 13:57:32.796046: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-30 13:57:32.796068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-30 13:57:32.796108: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-30 13:57:32.796131: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-30 13:57:32.796153: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-30 13:57:32.796194: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-10-30 13:57:32.796219: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-10-30 13:57:32.796326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-30 13:57:32.797146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-30 13:57:32.797792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-30 13:57:32.798701: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-30 13:57:34.470596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-10-30 13:57:34.470643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-10-30 13:57:34.470653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-10-30 13:57:34.472842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-30 13:57:34.473561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-30 13:57:34.474222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-30 13:57:34.474777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14915 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "2021-10-30 13:57:37.105108: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 603600000 exceeds 10% of free system memory.\n",
      "2021-10-30 13:57:37.602921: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-10-30 13:57:37.612397: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000129999 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.001, shape=(), dtype=float32).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-30 13:57:46.839643: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-30 13:57:47.606635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-30 13:57:47.661869: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - ETA: 0s - loss: 4.8291"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-30 13:58:08.341502: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 603600000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 39s 652ms/step - loss: 4.7861 - val_loss: 1.7408\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009995119, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 1.4662 - val_loss: 0.9858\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009990239, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.9845 - val_loss: 0.8080\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009985362, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.7744 - val_loss: 0.6809\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009980488, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.7103 - val_loss: 0.6976\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009975616, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.6792 - val_loss: 0.6385\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009970746, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.6528 - val_loss: 0.6248\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009965879, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.5798 - val_loss: 0.5395\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009961014, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.5365 - val_loss: 0.5073\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995615, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.5177 - val_loss: 0.5219\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995129, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.5200 - val_loss: 0.5829\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009946432, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.5199 - val_loss: 0.4711\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009941576, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.4789 - val_loss: 0.4816\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009936724, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.4578 - val_loss: 0.4639\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009931872, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.4439 - val_loss: 0.4330\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009927024, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.4207 - val_loss: 0.4256\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009922179, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.4232 - val_loss: 0.4422\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009917334, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.4186 - val_loss: 0.4470\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009912493, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.4095 - val_loss: 0.3870\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009907655, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3761 - val_loss: 0.4452\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009902817, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3928 - val_loss: 0.3860\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009897983, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3739 - val_loss: 0.3747\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009893151, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.3779 - val_loss: 0.3619\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009888322, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.3654 - val_loss: 0.3833\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009883496, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.3600 - val_loss: 0.3674\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.000987867, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.3718 - val_loss: 0.3970\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009873848, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.3736 - val_loss: 0.3508\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009869029, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3392 - val_loss: 0.3481\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.000986421, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3438 - val_loss: 0.3485\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009859394, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.3411 - val_loss: 0.3418\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009854581, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3319 - val_loss: 0.3595\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.000984977, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.3348 - val_loss: 0.3714\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009844962, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3394 - val_loss: 0.3586\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009840155, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.3581 - val_loss: 0.3329\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009835353, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3342 - val_loss: 0.3255\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009830552, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3089 - val_loss: 0.3281\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009825752, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3265 - val_loss: 0.3115\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009820956, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2987 - val_loss: 0.3108\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009816162, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.2992 - val_loss: 0.3253\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009811369, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.3010 - val_loss: 0.3221\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.000980658, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.3020 - val_loss: 0.3327\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009801793, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3063 - val_loss: 0.3070\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009797007, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2912 - val_loss: 0.3023\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009792225, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.3058 - val_loss: 0.3053\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009787445, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2815 - val_loss: 0.3328\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009782667, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3002 - val_loss: 0.2910\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009777892, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2846 - val_loss: 0.3070\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009773117, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2953 - val_loss: 0.2959\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009768347, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2759 - val_loss: 0.2956\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097635784, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 549ms/step - loss: 0.2832 - val_loss: 0.2966\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009758812, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2762 - val_loss: 0.2853\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097540487, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2785 - val_loss: 0.2992\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009749286, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.2728 - val_loss: 0.2943\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009744527, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2816 - val_loss: 0.3005\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009739771, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2736 - val_loss: 0.2834\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097350153, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2636 - val_loss: 0.3093\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009730263, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.2784 - val_loss: 0.2991\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097255135, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2767 - val_loss: 0.2891\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009720765, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2631 - val_loss: 0.2700\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.000971602, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2558 - val_loss: 0.2747\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009711277, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2551 - val_loss: 0.2736\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097065367, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.2645 - val_loss: 0.2933\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097017986, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.2617 - val_loss: 0.2903\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096970616, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2562 - val_loss: 0.2691\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096923276, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2467 - val_loss: 0.2696\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009687597, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2513 - val_loss: 0.2716\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096828677, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2491 - val_loss: 0.2689\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096781406, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2428 - val_loss: 0.2593\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096734165, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2519 - val_loss: 0.2693\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096686935, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2440 - val_loss: 0.2658\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009663974, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2418 - val_loss: 0.2677\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009659257, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2428 - val_loss: 0.2759\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009654541, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2500 - val_loss: 0.2685\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009649828, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2486 - val_loss: 0.2539\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096451165, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2339 - val_loss: 0.2723\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096404087, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2341 - val_loss: 0.2516\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096357026, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2351 - val_loss: 0.2829\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009630998, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2395 - val_loss: 0.2809\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096262974, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2341 - val_loss: 0.2470\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009621598, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2247 - val_loss: 0.2533\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096169004, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2345 - val_loss: 0.2720\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009612206, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2329 - val_loss: 0.2488\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009607513, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2405 - val_loss: 0.2734\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009602824, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2407 - val_loss: 0.3410\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009598136, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2986 - val_loss: 0.2846\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to tf.Tensor(0.000959345, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2483 - val_loss: 0.2736\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095887674, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2494 - val_loss: 0.2612\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095840864, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2354 - val_loss: 0.2782\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095794076, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2449 - val_loss: 0.2609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-30 14:27:37.448502: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 804800000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 2 < ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-30 14:28:21.461596: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 603600000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.001, shape=(), dtype=float32).\n",
      "37/37 [==============================] - ETA: 0s - loss: 5.0587"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-30 14:28:47.152495: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 603600000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 34s 648ms/step - loss: 5.0141 - val_loss: 1.7571\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009995119, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 1.4900 - val_loss: 0.9898\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009990239, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 1.0108 - val_loss: 0.7756\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009985362, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.8039 - val_loss: 0.7315\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009980488, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.7201 - val_loss: 0.6649\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009975616, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.6464 - val_loss: 0.6850\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009970746, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.6786 - val_loss: 0.5602\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009965879, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.5605 - val_loss: 0.6042\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009961014, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.5466 - val_loss: 0.5136\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995615, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.5127 - val_loss: 0.4818\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995129, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.5222 - val_loss: 0.4938\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009946432, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.4836 - val_loss: 0.4592\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009941576, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.4669 - val_loss: 0.4940\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009936724, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.4705 - val_loss: 0.4892\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009931872, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.4835 - val_loss: 0.4367\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009927024, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.4192 - val_loss: 0.4252\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009922179, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.4252 - val_loss: 0.4346\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009917334, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.4105 - val_loss: 0.4275\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009912493, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.4122 - val_loss: 0.4363\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009907655, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3934 - val_loss: 0.4191\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009902817, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3883 - val_loss: 0.3810\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009897983, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3760 - val_loss: 0.3831\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009893151, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3673 - val_loss: 0.4178\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009888322, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3873 - val_loss: 0.3700\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009883496, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3655 - val_loss: 0.4113\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.000987867, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3581 - val_loss: 0.3621\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009873848, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3641 - val_loss: 0.3817\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009869029, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3485 - val_loss: 0.3586\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.000986421, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3413 - val_loss: 0.3670\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009859394, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3502 - val_loss: 0.3808\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009854581, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3444 - val_loss: 0.3568\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.000984977, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3384 - val_loss: 0.3482\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009844962, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.3299 - val_loss: 0.3773\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009840155, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3226 - val_loss: 0.3308\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009835353, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.3199 - val_loss: 0.3340\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009830552, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.3169 - val_loss: 0.3223\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009825752, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3082 - val_loss: 0.3221\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009820956, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3096 - val_loss: 0.3180\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009816162, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3072 - val_loss: 0.3207\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009811369, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.3094 - val_loss: 0.3149\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.000980658, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3038 - val_loss: 0.3311\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009801793, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3157 - val_loss: 0.3233\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009797007, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3116 - val_loss: 0.3092\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009792225, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2907 - val_loss: 0.3039\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009787445, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2979 - val_loss: 0.3210\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009782667, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2997 - val_loss: 0.3364\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009777892, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2968 - val_loss: 0.3108\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009773117, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.3063 - val_loss: 0.3257\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009768347, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.2952 - val_loss: 0.3177\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097635784, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2830 - val_loss: 0.2923\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009758812, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2750 - val_loss: 0.3034\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097540487, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2741 - val_loss: 0.3328\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009749286, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2862 - val_loss: 0.2895\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009744527, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2768 - val_loss: 0.2930\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009739771, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2725 - val_loss: 0.3009\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097350153, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.2873 - val_loss: 0.3287\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009730263, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.3009 - val_loss: 0.3084\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097255135, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2772 - val_loss: 0.2845\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009720765, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.2638 - val_loss: 0.2875\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.000971602, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2648 - val_loss: 0.2779\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009711277, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 548ms/step - loss: 0.2597 - val_loss: 0.2946\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097065367, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.2645 - val_loss: 0.2768\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097017986, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2536 - val_loss: 0.2932\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096970616, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2594 - val_loss: 0.2749\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096923276, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2577 - val_loss: 0.2754\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009687597, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2589 - val_loss: 0.3090\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096828677, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2605 - val_loss: 0.3013\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096781406, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2495 - val_loss: 0.2661\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096734165, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2435 - val_loss: 0.2687\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096686935, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.2597 - val_loss: 0.2715\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009663974, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2411 - val_loss: 0.2626\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009659257, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2379 - val_loss: 0.2740\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009654541, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2469 - val_loss: 0.2684\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009649828, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2409 - val_loss: 0.2723\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096451165, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2400 - val_loss: 0.2637\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096404087, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2460 - val_loss: 0.2819\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096357026, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2419 - val_loss: 0.2580\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009630998, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2430 - val_loss: 0.2635\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096262974, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2392 - val_loss: 0.2639\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009621598, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2338 - val_loss: 0.2611\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to tf.Tensor(0.00096169004, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2291 - val_loss: 0.2622\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009612206, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2382 - val_loss: 0.2739\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009607513, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2391 - val_loss: 0.2533\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009602824, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2301 - val_loss: 0.2465\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009598136, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2288 - val_loss: 0.2585\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to tf.Tensor(0.000959345, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2341 - val_loss: 0.2690\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095887674, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2428 - val_loss: 0.2732\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095840864, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2367 - val_loss: 0.2678\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095794076, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2300 - val_loss: 0.2595\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009574731, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.2251 - val_loss: 0.2444\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009570057, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2199 - val_loss: 0.2502\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009565385, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2178 - val_loss: 0.2514\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009560716, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2239 - val_loss: 0.2514\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095560483, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2201 - val_loss: 0.2482\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095513836, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2142 - val_loss: 0.2385\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009546721, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.2141 - val_loss: 0.2451\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095420604, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2292 - val_loss: 0.2675\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009537402, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2257 - val_loss: 0.2508\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095327466, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2131 - val_loss: 0.2414\n",
      "Epoch 100/200\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095280923, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2137 - val_loss: 0.2627\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095234415, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.2214 - val_loss: 0.2856\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095187925, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2237 - val_loss: 0.2372\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009514145, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2080 - val_loss: 0.2471\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095095014, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2117 - val_loss: 0.2487\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009504858, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2030 - val_loss: 0.2378\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095002184, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 548ms/step - loss: 0.2019 - val_loss: 0.2608\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009495581, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.2040 - val_loss: 0.2356\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to tf.Tensor(0.00094909454, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2011 - val_loss: 0.2364\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009486312, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 543ms/step - loss: 0.2041 - val_loss: 0.3143\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009481682, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2515 - val_loss: 0.2496\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to tf.Tensor(0.00094770524, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2232 - val_loss: 0.2593\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009472426, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2225 - val_loss: 0.2439\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to tf.Tensor(0.00094678026, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.2156 - val_loss: 0.2499\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to tf.Tensor(0.00094631803, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 542ms/step - loss: 0.2138 - val_loss: 0.2444\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009458561, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 545ms/step - loss: 0.2081 - val_loss: 0.2697\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to tf.Tensor(0.00094539433, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 546ms/step - loss: 0.2266 - val_loss: 0.2535\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to tf.Tensor(0.00094493286, shape=(), dtype=float32).\n",
      "37/37 [==============================] - 20s 544ms/step - loss: 0.2186 - val_loss: 0.2766\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 200\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "kf = KFold(n_splits = 2, shuffle = True, random_state = 228)\n",
    "test_preds = []\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
    "    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
    "    X_train, X_valid = train[train_idx], train[test_idx]\n",
    "    y_train, y_valid = targets[train_idx], targets[test_idx]\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Input(shape = train.shape[-2:]),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(265, return_sequences = True)),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences = True)),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences = True)),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences = True)),\n",
    "        keras.layers.Dense(128, activation = 'selu'),\n",
    "        keras.layers.Dense(64, activation = 'selu'),\n",
    "        keras.layers.Dense(1),\n",
    "    ])\n",
    "    model.compile(optimizer = \"adam\", loss = \"mae\")\n",
    "\n",
    "    scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)/BATCH_SIZE), 1e-5)\n",
    "    lr = LearningRateScheduler(scheduler, verbose = 1)\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = EPOCH, batch_size = BATCH_SIZE, callbacks = [lr, callback])\n",
    "\n",
    "    test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f5aaa36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-30T15:08:16.748962Z",
     "iopub.status.busy": "2021-10-30T15:08:16.747703Z",
     "iopub.status.idle": "2021-10-30T15:08:27.049699Z",
     "shell.execute_reply": "2021-10-30T15:08:27.050233Z",
     "shell.execute_reply.started": "2021-10-30T08:48:05.986106Z"
    },
    "papermill": {
     "duration": 12.466647,
     "end_time": "2021-10-30T15:08:27.050392",
     "exception": false,
     "start_time": "2021-10-30T15:08:14.583745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss['pressure'] = sum(test_preds) / 2\n",
    "ss.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4352.362954,
   "end_time": "2021-10-30T15:08:32.828306",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-30T13:56:00.465352",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
